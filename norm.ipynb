{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6117a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy \n",
    "from konlpy.tag import Okt \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115b6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '.', '나', '는', '하늘색', '과', '딸기', '를', '좋', '아', '하', '어']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum=Hannanum()\n",
    "print(hannanum.morphs('안녕. 나는 하늘색과 딸기를 좋아해'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 트윗터 패키지 안에 konlpy 모듈호출\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "# 2.파일 읽기\n",
    "DEST = \"./processed_data/\"\n",
    "comp_list=[\"compatibility.csv\",\"incompatibility.csv\"]\n",
    "noun_list = []\n",
    "for filename in comp_list:\n",
    "    f = open(DEST+filename,'r',encoding='utf-8')\n",
    "    rdr = csv.reader(f)\n",
    "\n",
    "    # 3. 변수 okja에 문장들 저장\n",
    "    okja=[]\n",
    "    food_names=[]\n",
    "    for line in rdr:\n",
    "        okja.append(line[2])\n",
    "        food_names.append(line[0])\n",
    "        food_names.append(line[1])\n",
    "    # 4. 각 문장별로 형태소 구분하기\n",
    "    sentences_tag = []\n",
    "    for sentence in okja:\n",
    "        morph = twitter.pos(sentence)\n",
    "        sentences_tag.append(morph)\n",
    "    #    print(morph)\n",
    "    #    print('-'*30)\n",
    "\n",
    "    # 5. 명사만 선별해 리스트에 담기\n",
    "   \n",
    "    for sentence1 in sentences_tag:\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun']:\n",
    "                noun_list.append(word)\n",
    "\n",
    "    # 음식이름은 빼고 구하기 (set의 차집합)\n",
    "    noun_list= list(set(noun_list)-set(food_names))\n",
    "    \n",
    "    # 6. 선별된 명사들 출력\n",
    "    for word in noun_list:\n",
    "        print(word)\n",
    "\n",
    "\n",
    "# 컬럼명 달아주기\n",
    "noun_list.insert(0,\"DISEASE\")\n",
    "\n",
    "# 엑셀에 담기\n",
    "dataframe = pd.DataFrame(noun_list)\n",
    "dataframe.to_csv(DEST+'diseases.csv', header=False, index=False,encoding='utf-8-sig')\n",
    "\n",
    "print(len(noun_list))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acd38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.20.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae72218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4ad2e262d33b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m341\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# 각 말뭉치에서 명사 리스트를 만든다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mnoun_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;31m# 문자열로 저장해야하기 때문에 join함수로 공백을 구분해 corpus에 append한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoun_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if __name__=='__main__':\n",
    "    t_file_name=open('C:/Users/USER/Desktop/Contents_List.txt','r',encoding='utf-8')\n",
    "    # contents 리스트를 불러와 title_list변수에 저장\n",
    "    title_list=[]\n",
    "    for line in t_file_name.readlines():\n",
    "        title_list.append(line[:-1])\n",
    "    t_file_name.close()\n",
    "    \n",
    "    dataset=pd.read_csv(\"C:/Users/USER/Desktop/pre_도쿄(네이버).csv\")\n",
    "    # 형태소별로 분류\n",
    "    tagger=Okt()\n",
    "    \n",
    "    for title in title_list:\n",
    "        # 각 타이틀에 대한 341개 문서의 DTM을 표현하기 위해 객체 선언\n",
    "        cv=CountVectorizer() \n",
    "        # 각 문서들의 말뭉치를 저장할 리스트 선언\n",
    "        corpus=[] \n",
    "        \n",
    "        for doc_num in range(341):\n",
    "            # 각 말뭉치에서 명사 리스트를 만든다\n",
    "            noun_list=tagger.nouns(dataset[title].loc[doc_num])\n",
    "            # 문자열로 저장해야하기 때문에 join함수로 공백을 구분해 corpus에 append한다.\n",
    "            corpus.append(''.join(noun_list))\n",
    "            # fit_transform 함수를 통해 DTM을 한번에 생성\n",
    "            DTM_Array=cv.fit_transform(corpus).toarray()\n",
    "            # feature_names 함수를 통해 DTM의 각 열이 어떤 단어에 해당하는지 알 수 있다\n",
    "            feature_names=cv.get_feature_names()\n",
    "            \n",
    "            # 추출해낸 데이터를 DataFrame 형식으로 변환\n",
    "            DTM_DataFrame=pd.DataFrame(DTM_Array, columns=feature_names)\n",
    "            \n",
    "            #최종적으로 DTM을 csv파일로 저장\n",
    "            DTM_DataFrame.to_csv(\"C:/Users/USER/Desktop/DTM.csv\", encoding='utf-8-sig')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9223ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
